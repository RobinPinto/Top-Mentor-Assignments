1.What is K-means Clustering?
K-means Clustering:
K-means clustering is an unsupervised machine learning algorithm used to partition a dataset into distinct clusters. 
It groups similar data points together while ensuring dissimilarity between points in different clusters.

2.How is the initial value of the centroids usually determined in K-means clustering?
The initial values of the centroids are usually determined in one of two ways:

Random Initialization: The centroids are randomly selected from the data points. 
This is a common and simple method but can lead to different clusterings in different runs due to the random nature of initialization.

K-means++ Initialization: This method selects the first centroid randomly from the data, 
and subsequent centroids are chosen with a higher probability of being far from existing centroids. 
It improves the  quality of the final clusters.

3.Briefly describe the steps involved in the K-means clustering algorithm.

The K-means clustering algorithm involves the following steps:
Initialization of K centroids.
Assignment of data points to the nearest centroid.
Update of centroids as the mean of data points assigned to each cluster.
Repeat steps 2 and 3 until no significant change in assignments.

Example:
import numpy as np
from sklearn.neighbors import KNeighborsClassifier

xtrain=np.array([ [170,65],[180,85],[165,55],[176,75],[160,60]])
ytrain=["Body Builder","Body Builder","Non-Body Builder","Body Builder","Non-Body Builder"]
knn=KNeighborsClassifier(n_neighbors=3)
knn.fit(xtrain,ytrain)
print("Tell me you friend name?")
name=input()
print("what is his height?")
height=int(input())
print("what is his weight?")
weight=int(input())

result=np.array([[height,weight]])
prediction=knn.predict(result)
print(f"Your friend {name} is {prediction}")

4.Which Python library provides a direct implementation of the K-means clustering algorithm?
The K-means clustering algorithm can be implemented in Python using libraries like scikit-learn. 
The KMeans class in scikit-learn provides a direct implementation of K-means clustering.
for example:
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3)  # Create a K-means model with 3 clusters
kmeans.fit(data) # Fit the model to your data

5.How can you determine the optimal number of clusters (K) for your dataset?

The Elbow Method: This method involves running the K-means clustering on the dataset for a range of values of K (say, from 1 to 10),
and for each value of K, calculate the total within-cluster sum of squares (WSS). 
Plot the WSS values against the number of clusters (K). 
The point where the plot starts to bend or form an elbow is considered as an indicator of the optimal number of clusters.

The Silhouette Method: This method measures how similar a point is to its own cluster (cohesion) compared to other clusters (separation). 
The silhouette value ranges between -1 and +1. 
A high silhouette value indicates that the point is well-matched to its own cluster and poorly matched to neighboring clusters. 
If most points have a high silhouette value, then the clustering configuration is appropriate. 
If many points have a low or negative silhouette value, then the clustering configuration may have too many or too few clusters.

Gap Statistic Method: This method compares the total intracluster variation for different values of K with their expected values under null reference distribution of the data. 
The optimal number of clusters is usually where the gap statistic reaches its maximum.

These methods can provide a more objective means to determine the optimal number of clusters for our dataset.


6.How does the K-means algorithm determine that it has converged?
The K-means algorithm determines that it has converged when the assignments of data points to clusters no longer change significantly, or when a predefined stopping criterion is met. 
In practice, this means that the centroids have stabilized, and data points are not switching clusters.


7.Which distance metric is commonly used to compute the similarity between data points and centroids in K-means?
The most commonly used distance metric to compute similarity between data points and centroids in K-means is the Euclidean distance.
Euclidean distance represents the shortest distance between two vectors. 
However, other distance metrics like Manhattan distance or cosine similarity can also be used depending on the data and the problem.

8.What are some limitations of the K-means clustering algorithm?
Some limitations of the K-means clustering algorithm include:
a)Sensitivity to initial centroid selection.
b)The need to specify the number of clusters (K) in advance.
c)Assumption of spherical, equally sized clusters, which may not hold in all cases.
d)Difficulty handling non-linear clusters or clusters with irregular shapes.
e)Potential convergence to local optima.

9.Given the following code, what will be the number of clusters formed?
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3)
kmeans.fit(data)

The code provided specifies that K-means should form 3 clusters (n_clusters=3) using the data provided in the data variable.

10.How can you visualize the clusters formed by K-means on a 2D dataset in Python?
You can visualize the clusters formed by K-means on a 2D dataset in Python using libraries like Matplotlib. 
After fitting the K-means model to your data, you can plot the data points with different colors for each cluster to visualize the clustering results.
